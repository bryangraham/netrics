{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63365c",
   "metadata": {},
   "source": [
    "# Sparse network asymptotics for logistic regression under possible misspecification: Monte Carlo experiments\n",
    "_September 2022_    \n",
    "Bryan S. Graham, UC - Berkeley, bgraham@econ.berkeley.edu\n",
    "\n",
    "This iPython Jupyter notebook reproduces the Monte Carlo results reported in the Appendix C of the revised version of my paper \"Sparse network asymptotics for logistic regression under possible misspecification.” \n",
    "\n",
    "The scripts below were written for Python 3.6. The Anaconda distribution of Python, available at https://www.continuum.io/downloads, comes bundled with all the scientific computing packages used in this notebook. The notebook additionally uses the _ipt_ and _netrics_ modules that I have created. They are available on my GitHub page (https://github.com/bryangraham).\n",
    "\n",
    "Please feel free to use and modify the material in this notebook for your own research purposes. All I ask is that you cite both the underlying research as well as this codebase (see below for a suggested citation). If you find any errors in what follows I would be happy to hear about them. While I am not able to provide meaningful support for potential users of this code, I am willing to answer questions when/if I have the bandwidth to so.\n",
    "\n",
    "**References:**\n",
    "\n",
    "Graham, Bryan S. (2020). \"Sparse network asymptotics for logistic regression under possible misspecification,” _CEMMAP Working Paper CWP51/20_ (Revised 2024).   \n",
    "\n",
    "**Suggested code citation:**  \n",
    "\n",
    "Graham, Bryan S. (2022). \"Sparse network asymptotics for logistic regression under possible misspecification: Monte Carlo experiments Python Jupyter notebook,\" (Version 1.0) [Computer program]. Available at http://bryangraham.github.io/econometrics/ (Accessed 20 September 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28edfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Python to plot all figures inline (i.e., not in a separate window)\n",
    "%matplotlib inline\n",
    "\n",
    "# Main scientific computing modules\n",
    "# Load library dependencies\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "# Import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# networkx module for the analysis of network data\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d02f1",
   "metadata": {},
   "source": [
    "Load the **netrics** package. The Python 2.7 version of this package is registered on [PyPi](https://pypi.python.org/pypi/netrics/), with a GitHub repository at https://github.com/bryangraham/netrics_py27. For an informal introduction to the package see this [blog post](http://bryangraham.github.io/econometrics/networks/2016/09/15/netrics-module.html). The blog post also includes links to additional resources. The Python 3.6 version of the package is still under development, but currently available code can be found on GitHub at https://github.com/bryangraham/netrics. Once a basic level of functionality and reliability is in place I will register it on PyPi. The **netrics** package uses some functionality from the **ipt** package; so this latter package is loaded as well. You can read more about the **ipt** package at this [blog post](http://bryangraham.github.io/econometrics/causal/inference/2016/05/15/IPT-module.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb478ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append location of netrics and ipt modules base directory to system path\n",
    "# NOTE: only required if permanent install not made (see comments above)\n",
    "import sys\n",
    "sys.path.append('/Users/bgraham/Dropbox/Sites/software/ipt/')\n",
    "sys.path.append('/Users/bgraham/Dropbox/Sites/software/netrics/')\n",
    "\n",
    "# Load ipt and netrics modules\n",
    "import ipt as ipt\n",
    "import netrics as netrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b242129",
   "metadata": {},
   "source": [
    "### Monte Carlo Design\n",
    "\n",
    "(NOTE: Some of the text in this markdown box is borrowed from the paper.)   \n",
    "\n",
    "For the Monte Carlo experiments I set the graphon, $h_{n}\\left(\\mu,W_{i},X_{j},A_{i},B_{j},V_{ij}\\right)$, equal to \n",
    "\n",
    "$$ Y_{ij}\t=1\\left(\\alpha+z\\left(W_{i},X_{j}\\right)'\\beta+\\ln\\left(A_{i}\\right)+\\ln\\left(B_{j}\\right)-\\ln\\left(n\\right)\\geq V_{ij}\\right) $$ \n",
    "\n",
    "with $V_{ij}$ a standard exponential random variable.   \n",
    "\n",
    "Averaging over $V_{ij}$ yields\n",
    "\n",
    "$$ \\mathbb{E}_{n}\\left[\\left.Y_{ij}\\right|W_{i},X_{j},A_{i},B_{j}\\right]=\\frac{1}{n}\\exp\\left(\\alpha+z\\left(W_{i},X_{j}\\right)'\\beta\\right)A_{i}B_{j}. $$ \n",
    "\n",
    "I set $\\left\\{ A_{i}\\right\\} _{i=1}^{N}$ and $\\left\\{ B_{j}\\right\\} _{j=1}^{M}$ to be iid log-normal sequences of random variables with $\\mu=-1/12$ and $\\sigma=1/\\sqrt{6}$. This implies that both $A_{i}$ and $B_{j}$ are mean one and, furthermore, that the variance of the sum $\\ln\\left(A_{i}\\right)+\\ln\\left(B_{j}\\right)$ is one third that of $V_{ij}$. This generates meaningful, but not overpowering, cross dyad dependence. Under these assumptions the regression function equals\n",
    "\n",
    "$$g_{n}\\left(w,x\\right)=\\frac{1}{n}\\exp\\left(\\alpha+z\\left(W_{i},X_{j}\\right)'\\beta\\right).$$\n",
    "\n",
    "Finally I set $z\\left(W_{i},X_{j}\\right)=\\left(\\begin{array}{ccc}\n",
    "W_{i} & X_{j} & W_{i}X_{j}\\end{array}\\right)'$ with $\\left\\{ W_{i}\\right\\} _{i=1}^{N}$ iid Bernouli with a success probability $\\pi_{w}=1/\\sqrt{3}$ and $\\left\\{ X_{j}\\right\\} _{j=1}^{M}$ iid Bernouli with a success probability $\\pi_{x}=1/\\sqrt{3}$. This implies that one third of dyads are of the $W_{i}=X_{j}=1$ type.\n",
    "\n",
    "I simulate data for five sampe sizes: $n=64, 144, 256, 576$ and $1024$ with $N=M$ in all cases. I set $\\alpha=\\ln\\left(64\\times0.04\\right)$, $\\beta_{w}=\\beta_{x}=0$ and $\\beta_{wx}=\\ln4\\approx1.3863$. This implies that $\\rho_{n}=0.08, 0.036, 0.020, 0.009$ and $0.005$ across the five designs. For each design I perform $5,000$ Monte Carlo replications.\n",
    "\n",
    "The design is stylized enough that the various score projections appearing in the main paper (see Equations (21) to (27) in the paper) can be computed analytically.\n",
    "\n",
    "Specifically we have a \"score\" equation of, maintaining the notation defined in the paper,\n",
    "\n",
    "$$ s_{ij,n}=\\left(Y_{ij}-e_{ij,n}\\right)R_{ij}. $$\n",
    "\n",
    "Averaging over $V_{ij}$ gives a first projection of   \n",
    "\n",
    "$$ \\bar{s}_{ij,n}=\\left(\\frac{1}{n}\\exp\\left(\\alpha+z\\left(W_{i},X_{j}\\right)'\\beta\\right)A_{i}B_{j}-e_{ij,n}\\right)R_{ij}. $$\n",
    "\n",
    "This term is the kernel of the U-Statistic discussed in the paper. The two terms which together define the Hajek projection of this U-Statistic are\n",
    "\n",
    "\n",
    "$$\\bar{s}_{1i,n}^{c}=\\left(1-\\pi_{x}\\right)\\left(\\frac{1}{n}\\exp\\left(\\alpha+W_{i}\\beta_{w}\\right)\\left(A_{i}-\\frac{1}{1+\\frac{1}{n}\\exp\\left(\\alpha+W_{i}\\beta_{w}\\right)}\\right)\\right)\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "W_{i}\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right)\n",
    "\t+\\pi_{x}\\left(\\frac{1}{n}\\exp\\left(\\alpha+W_{i}\\beta_{w}+\\beta_{x}+W_{i}\\beta_{wx}\\right)\\left(A_{i}-\\frac{1}{1+\\frac{1}{n}\\exp\\left(\\alpha+W_{i}\\beta_{w}+\\beta_{x}+W_{i}\\beta_{wx}\\right)}\\right)\\right)\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "W_{i}\\\\\n",
    "1\\\\\n",
    "W_{i}\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\bar{s}_{1j,n}^{p}=\t\\left(1-\\pi_{w}\\right)\\left(\\frac{1}{n}\\exp\\left(\\alpha+X_{j}\\beta_{x}\\right)\\left(B_{j}-\\frac{1}{1+\\frac{1}{n}\\exp\\left(\\alpha+X_{j}\\beta_{x}\\right)}\\right)\\right)\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "X_{j}\\\\\n",
    "0\n",
    "\\end{array}\\right)\n",
    "\t+\\pi_{w}\\left(\\frac{1}{n}\\exp\\left(\\alpha+\\beta_{w}+X_{j}\\beta_{x}+X_{j}\\beta_{wx}\\right)\\left(B_{j}-\\frac{1}{1+\\frac{1}{n}\\exp\\left(\\frac{1}{n}\\exp\\left(\\alpha+\\beta_{w}+X_{j}\\beta_{x}+X_{j}\\beta_{wx}\\right)\\right)}\\right)\\right)\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "1\\\\\n",
    "X_{j}\\\\\n",
    "X_{j}\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "These terms can be used to compute $S_n$, $U_{1n}$, $U_{2n}$ and $V_{n}$ in \"closed form\". The variances of these terms can be computed using the sample variance across the $5,000$ Monte Carlo replications. **Table 2** in the paper examiness the properties of these \"score\" components in detail; verifying the rate-of-convergence calculations in the paper.\n",
    "\n",
    "The bias term of $S_{n}$ is as given in the paper in equation (27). However the first term in this equation is zero for the design considered here because $\\lambda \\left(w,x\\right)$ takes an exponential form (which is consistent with the limiting form of the logit function).\n",
    "\n",
    "The next block of code executes the Monte Carlo simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c7970a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo design 1 of 5\n",
      "Number of consumers and products, n = N + M : 64\n",
      "Marginal purchase probability               : 0.080\n",
      "Bias in pseudo-score vector n^(3/2)*S_n     : 3.766\n",
      "(Above bias is for the component of S_n corresponding to the interaction coefficient)\n",
      "Time required f/ MC rep  500 of 500: 0.016544818878173828\n",
      "Monte Carlo design 2 of 5\n",
      "Number of consumers and products, n = N + M : 144\n",
      "Marginal purchase probability               : 0.036\n",
      "Bias in pseudo-score vector n^(3/2)*S_n     : 2.719\n",
      "(Above bias is for the component of S_n corresponding to the interaction coefficient)\n",
      "Time required f/ MC rep  500 of 500: 0.027776002883911133\n",
      "Monte Carlo design 3 of 5\n",
      "Number of consumers and products, n = N + M : 256\n",
      "Marginal purchase probability               : 0.020\n",
      "Bias in pseudo-score vector n^(3/2)*S_n     : 2.101\n",
      "(Above bias is for the component of S_n corresponding to the interaction coefficient)\n",
      "Time required f/ MC rep  500 of 500: 0.0498199462890625\n",
      "Monte Carlo design 4 of 5\n",
      "Number of consumers and products, n = N + M : 576\n",
      "Marginal purchase probability               : 0.009\n",
      "Bias in pseudo-score vector n^(3/2)*S_n     : 1.431\n",
      "(Above bias is for the component of S_n corresponding to the interaction coefficient)\n",
      "Time required f/ MC rep  500 of 500: 0.487515926361084\n",
      "Monte Carlo design 5 of 5\n",
      "Number of consumers and products, n = N + M : 1024\n",
      "Marginal purchase probability               : 0.005\n",
      "Bias in pseudo-score vector n^(3/2)*S_n     : 1.081\n",
      "(Above bias is for the component of S_n corresponding to the interaction coefficient)\n",
      "Time required f/ MC rep  500 of 500: 1.8961951732635498\n"
     ]
    }
   ],
   "source": [
    "# N,M = 32, 72, 128, 288, 512, 1152, 2048 sqrt(n) = 8, 12, 16, 24, 32, 48, 64\n",
    "# (simulated sample sizes and relative magnitudes in \"rate-of-convergence\" terms\")\n",
    "\n",
    "designs = [32, 72, 128, 288, 512]\n",
    "NumDesigns = len(designs)\n",
    "\n",
    " # Number of Monte Carlo simulations\n",
    "S = 500               \n",
    "\n",
    "#----------------------------------------------------#\n",
    "#- CORE FEATURES OF MONTE CARLO DESIGNS             -#\n",
    "#----------------------------------------------------#\n",
    "    \n",
    "n0 = 64                 # Initial sample size used to calibrate intercept, alpha\n",
    "\n",
    "mu    = -1/12           # Mean and variance of consumer and product effects\n",
    "sigma =  1/np.sqrt(6)   # Variance of ln(A_i) + ln(B_j) = 2sigma^2\n",
    "\n",
    "pi_w  = (1/3)**(1/2)    # Covariate distribution parameters\n",
    "pi_x  = (1/3)**(1/2)    # One third transaction opportunities are W_i=X_j=1 cases.\n",
    "\n",
    "# Initialize matrices for storage of Monte Carlo results\n",
    "theta_hat = np.zeros((S,NumDesigns))\n",
    "coverage  = np.zeros((S,NumDesigns*4))\n",
    "se_hat    = np.zeros((S,NumDesigns*4))\n",
    "\n",
    "# Matrices to store the components of \"score\" vector\n",
    "S_n       = np.zeros((S,NumDesigns*4))\n",
    "U1_n      = np.zeros((S,NumDesigns*4))\n",
    "U2_n      = np.zeros((S,NumDesigns*4))\n",
    "V_n       = np.zeros((S,NumDesigns*4))\n",
    "\n",
    "\n",
    "#----------------------------------------------------#\n",
    "#- BEGIN MONTE CARLO SIMULATIONS.                   -#\n",
    "#----------------------------------------------------#\n",
    "\n",
    "for b in range(0,NumDesigns ):\n",
    "    \n",
    "    # Set random seed and start bth Monte Carlo experiments\n",
    "    # (same seed is used for each design)\n",
    "    np.random.seed(seed=361)\n",
    "    \n",
    "    print(\"Monte Carlo design \" + '%.0f' % (b+1) + \" of \" + '%.0f' % NumDesigns )\n",
    "\n",
    "    #----------------------------------------------------#\n",
    "    #- DEFINE DATA GENERATING PROCESS FOR MONTE CARLO   -#\n",
    "    #- MISPECIFIED CASE: \"EXOBIT\"                       -#\n",
    "    #----------------------------------------------------#\n",
    "\n",
    "    N = designs[b]                 # Number of consumers\n",
    "    M = designs[b]                 # Number of products\n",
    "    n = N + M                      # Standard error should decrease at rate 1/sqrt(n)\n",
    "    \n",
    "    # Parameters indexing regression function\n",
    "    theta = np.array([0, 0, np.log(4), np.log(n0*0.04)])\n",
    "\n",
    "    # Marginal purchase probability for current design, rho_n\n",
    "    rho0_n   = (1 - pi_w) * (1 - pi_x) * (1/n)*np.exp(theta[3])+ \\\n",
    "               (1 - pi_w) * pi_x       * (1/n)*np.exp(theta[1] + theta[3]) + \\\n",
    "                     pi_w * (1 - pi_x) * (1/n)*np.exp(theta[0] + theta[3]) + \\\n",
    "                     pi_w * pi_x       * (1/n)*np.exp(theta[0] + theta[1] + theta[2] + theta[3]) \n",
    "\n",
    "    print('Number of consumers and products, n = N + M : ' + '%.0f' % n)\n",
    "    print('Marginal purchase probability               : ' + '%.3f' % rho0_n)\n",
    "\n",
    "    # Computed fixed n bias of the score vector, b_n(theta)\n",
    "    b_n   = (1 - pi_w) * (1 - pi_x) * (1/n)*np.exp(theta[3]) * \\\n",
    "                                      (1 - 1/(1 +(1/n)*np.exp(theta[3]))) * np.array([0, 0, 0, 1]) + \\\n",
    "            (1 - pi_w) * pi_x       * (1/n)*np.exp(theta[1] + theta[3]) * \\\n",
    "                                      (1 - 1/(1 +(1/n)*np.exp(theta[1] + theta[3]))) * np.array([0, 1, 0, 1]) + \\\n",
    "                  pi_w * (1 - pi_x) * (1/n)*np.exp(theta[0] + theta[3]) * \\\n",
    "                                      (1 - 1/(1 +(1/n)*np.exp(theta[0] + theta[3]))) * np.array([1, 0, 0, 1]) + \\\n",
    "                  pi_w * pi_x       * (1/n)*np.exp(theta[0] + theta[1] + theta[2] + theta[3]) * \\\n",
    "                                      (1 - 1/(1 +(1/n)*np.exp(theta[0] + theta[1] + theta[2] + theta[3]))) \\\n",
    "                                    * np.array([1, 1, 1, 1])\n",
    "    print('Bias in pseudo-score vector n^(3/2)*S_n     : ' + '%.3f' %  (n**(3/2)*b_n[2]))\n",
    "    print('(Above bias is for the component of S_n corresponding to the interaction coefficient)')\n",
    "    \n",
    "    # Compute the limiting value of normalized/rescaled Hessian matrix (nH)\n",
    "    GAMMA = (1 - pi_w) * (1 - pi_x) * np.exp(theta[3]) * \\\n",
    "                                      np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]]) +\\\n",
    "                  pi_w * (1 - pi_x) * np.exp(theta[0] + theta[3]) * \\\n",
    "                                      np.array([[1, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 1]]) +\\\n",
    "            (1 - pi_w) *       pi_x * np.exp(theta[1] + theta[3]) * \\\n",
    "                                      np.array([[0, 0, 0, 0], [0, 1, 0, 1], [0, 0, 0, 0], [0, 1, 0, 1]]) +\\\n",
    "                  pi_w *       pi_x * np.exp(theta[0] + theta[1] + theta[1] + theta[2]) * \\\n",
    "                                      np.array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])\n",
    "\n",
    "    # Compute the inverse\n",
    "    iGAMMA = np.linalg.inv(GAMMA)\n",
    "\n",
    "    #----------------------------------------------#\n",
    "    #- MONTE CARLO SIMULATIONS FOR CURRENT DESIGN -#\n",
    "    #----------------------------------------------#\n",
    "\n",
    "    for s in range(0,S):\n",
    "        start = time.time()\n",
    "    \n",
    "        #-------------------------------------#\n",
    "        #- STEP 1 : SIMULATE BIPARTITE GRAPH -#\n",
    "        #-------------------------------------#\n",
    "    \n",
    "        # Simulate consumer and product heterogeneity/effects\n",
    "        A = np.random.lognormal(mu, sigma, N)\n",
    "        B = np.random.lognormal(mu, sigma, M)\n",
    "    \n",
    "        # Simulate consumer and product covariates\n",
    "        W = np.random.binomial(1, pi_w, N)\n",
    "        X = np.random.binomial(1, pi_x, M)\n",
    "    \n",
    "        # Generate s-th Monte Carlo draw of Y\n",
    "        WX_n       = np.kron(W,X) \n",
    "    \n",
    "        iota_M   = np.ones((M,))\n",
    "        i        = np.kron(np.arange(0,N), iota_M)\n",
    "        W_n      = np.kron(W, iota_M) \n",
    "        A_n      = np.kron(A, iota_M) \n",
    "    \n",
    "        iota_N   = np.ones((N,))\n",
    "        j        = np.kron(iota_N, np.arange(0,M))\n",
    "        X_n      = np.kron(iota_N, X) \n",
    "        B_n      = np.kron(iota_N, B) \n",
    "    \n",
    "        Y_n      = 1*(-theta[0]*W_n - theta[1]*X_n - theta[2]*WX_n - theta[3] \\\n",
    "                      - np.log(A_n) - np.log(B_n) + np.log(n) <= np.random.exponential(1, (N*M,)))\n",
    "    \n",
    "        # Mean of Y_ij given W_i, X_j, A_i and B_j\n",
    "        hbar_n   = (1/n)*(np.exp(theta[0]*W_n + theta[1]*X_n + theta[2]*WX_n + theta[3])*A_n*B_n)\n",
    "    \n",
    "        # Logit function at \"true\" theta\n",
    "        e_n      =  ((1/n)*np.exp(theta[0]*W_n + theta[1]*X_n + theta[2]*WX_n + theta[3]) / \\\n",
    "                 (1 + (1/n)*np.exp(theta[0]*W_n + theta[1]*X_n + theta[2]*WX_n + theta[3])))\n",
    "    \n",
    "        # Form regressor matrix as pandas dataframe for using in bilogit() command\n",
    "        R = pd.DataFrame(np.vstack([Y_n, hbar_n, e_n, W_n, X_n, WX_n, i, j]).T, \\\n",
    "                         columns=['Y_ij', 'hbar_ij', 'e_ij', 'W_i', 'X_j', 'W_i x X_j', 'i', 'j'])    \n",
    "        R = R.set_index(['i', 'j'], drop = True)  # Set dataframe multi-indexr\n",
    "        R['constant'] = 1\n",
    "\n",
    "        # Get outcome variable and drop from regressor matrix\n",
    "        Y = R['Y_ij'].copy(deep=True)\n",
    "        R.drop('Y_ij', axis=1, inplace=True)\n",
    "    \n",
    "        # Get expected outcome variable given W_i, X_j, A_i and B_j and drop from regressor matrix\n",
    "        hbar_n = R['hbar_ij'].copy(deep=True)\n",
    "        R.drop('hbar_ij', axis=1, inplace=True)\n",
    "    \n",
    "        # Get modeled expected outcome variable given W_i and X_j, anddrop from regressor matrix\n",
    "        e_n = R['e_ij'].copy(deep=True)\n",
    "        R.drop('e_ij', axis=1, inplace=True)\n",
    "    \n",
    "        #------------------------------------------#\n",
    "        #- STEP 2 : CALCULATE SCORE DECOMPOSITION -#\n",
    "        #------------------------------------------#\n",
    "    \n",
    "        # Score vector\n",
    "        s_n =    (1/(N*M))*(R.to_numpy().T @ (Y - e_n).to_numpy().reshape(-1,1))\n",
    "    \n",
    "        # Mean if score conditional of W, X, A, and B (U-Statistics Kernel)\n",
    "        sbar_n = (1/(N*M))*(R.to_numpy().T @ (hbar_n - e_n).to_numpy().reshape(-1,1))\n",
    "    \n",
    "        # First projections\n",
    "        # s1c_n\n",
    "        t1 = np.vstack([W, np.zeros((N,)), np.zeros((N,)), np.ones((N,))])\n",
    "        t2 = (1 - pi_x) * (1/n)*np.exp(theta[0]*W + theta[3]) * \\\n",
    "                         (A - 1/(1 +(1/n)*np.exp(theta[0]*W + theta[3])))\n",
    "        t3 = np.vstack([W, np.ones((N,)), W, np.ones((N,))])\n",
    "        t4 = pi_x       * (1/n)*np.exp(theta[0]*W + theta[1] + theta[2]*W + theta[3]) * \\\n",
    "                         (A - 1/(1 +(1/n)*np.exp(theta[0]*W + theta[1] + theta[2]*W + theta[3]))) \n",
    "        s1c_n = (t1*t2 + t3*t4).T\n",
    "    \n",
    "        # s1p_n\n",
    "        t1 = np.vstack([np.zeros((M,)), X, np.zeros((M,)), np.ones((M,))])\n",
    "        t2 = (1 - pi_w) * (1/n)*np.exp(theta[1]*X + theta[3]) * \\\n",
    "                          (B - 1/(1 +(1/n)*np.exp(theta[1]*X + theta[3]))) \n",
    "        t3 = np.vstack([np.ones((M,)), X, X, np.ones((M,))])\n",
    "        t4 = pi_w       * (1/n)*np.exp(theta[0] + theta[1]*X + theta[2]*X + theta[3]) * \\\n",
    "                          (B - 1/(1 +(1/n)*np.exp(theta[0] + theta[1]*X + theta[2]*X + theta[3])))\n",
    "        s1p_n = (t1*t2 + t3*t4).T\n",
    "    \n",
    "        # Construct components of score decomposition (population values)\n",
    "        S_n[s,(b*4):((b+1)*4)]  = s_n.T - b_n\n",
    "        U1_n[s,(b*4):((b+1)*4)] = np.mean(s1c_n - b_n, axis=0) + np.mean(s1p_n - b_n, axis=0)\n",
    "        U2_n[s,(b*4):((b+1)*4)] = (sbar_n.T - b_n) - U1_n[s,(b*4):((b+1)*4)]\n",
    "        V_n[s,(b*4):((b+1)*4)]  = s_n.T - sbar_n.T\n",
    "    \n",
    "        #----------------------------------------------------------#\n",
    "        #- STEP 3 : COMPUTE PSEUDO COMPOSITE LIKELIHOOD ESTIMATES -#\n",
    "        #----------------------------------------------------------#\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #- Estimation is with the bilogit() command included in the netrics module-#\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        # (i) Dense network standard errors\n",
    "        # ---------------------------------\n",
    "        \n",
    "        [theta_BL, vcov_theta_BL]= netrics.bilogit(Y, R, nocons=True, silent=True, cov='dense')\n",
    "    \n",
    "        # Adjust intercept for -ln(n) in theoretical approximating sequence\n",
    "        theta_BL[3,0] = theta_BL[3,0] + np.log(n)\n",
    "    \n",
    "        # Save pseudo composite MLE of interaction coefficient\n",
    "        theta_hat[s,b] = theta_BL[2,0]\n",
    "    \n",
    "        # See if true interaction coefficient is inside dense Wald-based confidence interval\n",
    "        coverage[s,b*4]  = (theta[2]<=theta_BL[2,0] + 1.96*np.sqrt(vcov_theta_BL[2,2]))*\\\n",
    "                           (theta[2]>=theta_BL[2,0] - 1.96*np.sqrt(vcov_theta_BL[2,2]))\n",
    "           \n",
    "        # Standard error length\n",
    "        se_hat[s,b*4]    = np.sqrt(vcov_theta_BL[2,2])\n",
    "    \n",
    "        # (ii) Bias correction / Sparse network standard errors\n",
    "        # -----------------------------------------------------\n",
    "        \n",
    "        [theta_BL, vcov_theta_BL]= netrics.bilogit(Y, R, nocons=True, silent=True, cov='sparse')\n",
    "  \n",
    "        # Adjust intercept for -ln(n) in theoretical approximating sequence\n",
    "        theta_BL[3,0] = theta_BL[3,0] + np.log(n)\n",
    "    \n",
    "        # See if true interaction coefficient is inside sparse Wald-based confidence interval\n",
    "        coverage[s,b*4+1]  = (theta[2]<=theta_BL[2,0] + 1.96*np.sqrt(vcov_theta_BL[2,2]))*\\\n",
    "                             (theta[2]>=theta_BL[2,0] - 1.96*np.sqrt(vcov_theta_BL[2,2]))\n",
    "               \n",
    "        # Standard error length\n",
    "        se_hat[s,b*4+1] = np.sqrt(vcov_theta_BL[2,2])\n",
    "    \n",
    "        end = time.time()\n",
    "        if (s+1) % 500 == 0:\n",
    "            print(\"Time required f/ MC rep  \" + str(s+1) + \" of \" + str(S) + \": \" + str(end-start))      \n",
    "\n",
    "    #-------------------------------------------------------------------------#\n",
    "    #- STEP 4 : ASSESS QUALITY OF SPARSE NETWORK APPROXIMATION THEORY        -#\n",
    "    #-------------------------------------------------------------------------#     \n",
    "    \n",
    "    # Compute variance of (population) score components by Monte Carlo integration\n",
    "    Var_U1  = np.cov(U1_n[:,(b*4):((b+1)*4)] , rowvar = False, ddof = 1)\n",
    "    Var_U1V = np.cov(U1_n[:,(b*4):((b+1)*4)] + V_n[:,(b*4):((b+1)*4)], rowvar = False, ddof = 1)\n",
    "\n",
    "    # Population asymptotic variance under dense and sparse regimes\n",
    "    vcov_theta_dense  = (iGAMMA @ Var_U1 @ iGAMMA)*(n**2)\n",
    "    vcov_theta_sparse = (iGAMMA @ Var_U1V @ iGAMMA)*(n**2)\n",
    "\n",
    "    # (Oracle) Coverage - dense case\n",
    "    coverage[:,b*4+2]  = (theta[2]<=theta_hat[:,b] + 1.96*np.sqrt(vcov_theta_dense[2,2]))*\\\n",
    "                         (theta[2]>=theta_hat[:,b] - 1.96*np.sqrt(vcov_theta_dense[2,2]))\n",
    "\n",
    "    # (Oracle) Standard error length - dense case\n",
    "    se_hat[:,b*4+2]      = np.sqrt(vcov_theta_dense[2,2])\n",
    "    \n",
    "    # (Oracle) Coverage - sparse case\n",
    "    coverage[:,b*4+3]  = (theta[2]<=theta_hat[:,b] + 1.96*np.sqrt(vcov_theta_sparse[2,2]))*\\\n",
    "                         (theta[2]>=theta_hat[:,b] - 1.96*np.sqrt(vcov_theta_sparse[2,2]))\n",
    "\n",
    "    se_hat[:,b*4+3]    = np.sqrt(vcov_theta_sparse[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc251cf3",
   "metadata": {},
   "source": [
    "### Monte Carlo Results\n",
    "\n",
    "The next few blocks of code print out the Monte Carlo simulation results which appear in **Tables 3** and **4** of the main paper (in Appendix C). First I print out the mean and median bias of the estimated interaction coefficient, $\\hat{\\beta}_{wx}$. This is in rows 1 and 2 below, with the different samples sizes corresponding to each column (from smallest to largest). I also compute the standard deviation of $\\hat{\\beta}_{wx}$ across the $5,000$ Monte Carlo replications. I use both the sample standard deviation (row 3) and a robust measure (row 4). The two estimates a very close except for the smallest sample size. The paper reports the robust measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "print(\"theta_0 : \" + '%.5f' % theta[2])\n",
    "print(np.mean(theta_hat-theta[2],axis=0))\n",
    "print(np.median(theta_hat-theta[2],axis=0))\n",
    "print(np.std(theta_hat,axis=0))\n",
    "print((np.quantile(theta_hat, q=0.95, axis=0)-np.quantile(theta_hat, q=0.05, axis=0))/(norm.ppf(0.95)-norm.ppf(0.05)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd3626",
   "metadata": {},
   "source": [
    "Next I report coverage estimates of the various confidence intervals. The first row reports the coverage of dense intervals for each design. The second reports the coverage of sparse intervals. These intervals are computed by the **bilogit()** command in the **netrics** module as described in an appendix to the paper. The final two rows report the coverage of infeasible dense and sparse oracle intervals (not reported in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8104db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.346 0.376 0.394 0.292 0.3  ]\n",
      " [0.884 0.926 0.96  0.948 0.946]\n",
      " [0.448 0.508 0.51  0.518 0.556]\n",
      " [0.924 0.964 0.968 0.974 0.984]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(coverage,axis=0).reshape((NumDesigns,4)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066b055",
   "metadata": {},
   "source": [
    "Next I report mean estimated standard errors for $\\hat{\\beta}_{wx}$. Results are organized in the same order as those for confidence intervals above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dae454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18893106 0.11262055 0.07470341 0.04316749 0.03007622]\n",
      " [0.60994083 0.40028239 0.2990095  0.19896063 0.14888062]\n",
      " [0.22212279 0.14398374 0.10381886 0.07905648 0.05619455]\n",
      " [0.68125364 0.4853297  0.32976185 0.23467324 0.17501981]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(se_hat,axis=0).reshape((NumDesigns,4)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3568962",
   "metadata": {},
   "source": [
    "### Population score decomposition\n",
    "\n",
    "The asymptotic theory in the paper hinges critically on the behavior of the \"score\" vector under sparse sequences. This next block of code prints out some basic summary statistics for the components of this \"score\". These statistics are reported in Table 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa0c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Mean and standard deviations of scaled score components: S_n, U1_n, U2_n, V_n, U1n+V_n, n = 256 design\n",
      "[ 0.03031  0.07608 -0.02339 -0.02237  0.05371]\n",
      "[5.10424 3.63756 0.30799 3.60838 5.11041]\n",
      " \n",
      "Tail frequences on standardized score components\n",
      "[0.066 0.052 0.028 0.048 0.068]\n",
      "[0.048 0.042 0.048 0.056 0.048]\n",
      "[0.038 0.024 0.026 0.028 0.034]\n",
      "[0.024 0.026 0.036 0.02  0.022]\n",
      " \n",
      "Mean and standard deviations of scaled score components: S_n, U1_n, U2_n, V_n, U1n+V_n, n = 1024 design\n",
      "[ 0.33108  0.31563 -0.00085  0.0163   0.33193]\n",
      "[5.51119 3.93246 0.16997 3.79615 5.51774]\n",
      " \n",
      "Tail frequences on standardized score components\n",
      "[0.048 0.054 0.054 0.046 0.05 ]\n",
      "[0.034 0.04  0.05  0.054 0.034]\n",
      "[0.024 0.034 0.034 0.022 0.024]\n",
      "[0.016 0.024 0.038 0.028 0.016]\n"
     ]
    }
   ],
   "source": [
    "tS_n    = S_n / np.diag(np.cov(S_n, rowvar = False, ddof = 1))**(1/2)\n",
    "tU1_n   = U1_n / np.diag(np.cov(U1_n, rowvar = False, ddof = 1))**(1/2)\n",
    "tU2_n   = U2_n / np.diag(np.cov(U2_n, rowvar = False, ddof = 1))**(1/2)\n",
    "tV_n    = V_n / np.diag(np.cov(V_n, rowvar = False, ddof = 1))**(1/2)\n",
    "tU1V_n  = (U1_n + V_n) / np.diag(np.cov(U1_n + V_n, rowvar = False, ddof = 1))**(1/2)\n",
    "\n",
    "np.set_printoptions(suppress=False, precision=5)\n",
    "\n",
    "#------------------------------------------------#\n",
    "#- CASE 1: n=256 design                         -#\n",
    "#------------------------------------------------#\n",
    "\n",
    "print(\" \")\n",
    "n=256\n",
    "\n",
    "print(\"Mean and standard deviations of scaled score components: S_n, U1_n, U2_n, V_n, U1n+V_n, n = 256 design\")\n",
    "print(n**(3/2)*np.mean(np.vstack([S_n[:,(NumDesigns-3)*4+2], U1_n[:,(NumDesigns-3)*4+2], U2_n[:,(NumDesigns-3)*4+2], \\\n",
    "                                  V_n[:,(NumDesigns-3)*4+2],  \\\n",
    "                                  U1_n[:,(NumDesigns-3)*4+2] + V_n[:,(NumDesigns-3)*4+2]]).T, axis=0))\n",
    "\n",
    "print(n**(3/2)*np.diag(np.cov(np.vstack([S_n[:,(NumDesigns-3)*4+2], U1_n[:,(NumDesigns-3)*4+2], U2_n[:,(NumDesigns-3)*4+2], \\\n",
    "                                         V_n[:,(NumDesigns-3)*4+2], \\\n",
    "                                         U1_n[:,(NumDesigns-3)*4+2] + V_n[:,(NumDesigns-3)*4+2]]).T, \\\n",
    "                              rowvar = False, ddof = 1))**(1/2))\n",
    "\n",
    "print(\" \")\n",
    "t = np.vstack([tS_n[:,(NumDesigns-3)*4+2], tU1_n[:,(NumDesigns-3)*4+2],  tU2_n[:,(NumDesigns-3)*4+2], \\\n",
    "               tV_n[:,(NumDesigns-3)*4+2], tU1V_n[:,(NumDesigns-3)*4+2]]).T\n",
    "\n",
    "print(\"Tail frequences on standardized score components\")\n",
    "print(np.mean(t>=1.6449, axis = 0))\n",
    "print(np.mean(t<=-1.6449, axis = 0))\n",
    "print(np.mean(t>=1.96, axis = 0))\n",
    "print(np.mean(t<=-1.96, axis = 0))\n",
    "\n",
    "#------------------------------------------------#\n",
    "#- CASE 2: n=1024 design                        -#\n",
    "#------------------------------------------------#\n",
    "\n",
    "print(\" \")\n",
    "n=1024\n",
    "\n",
    "print(\"Mean and standard deviations of scaled score components: S_n, U1_n, U2_n, V_n, U1n+V_n, n = 1024 design\")\n",
    "print(n**(3/2)*np.mean(np.vstack([S_n[:,(NumDesigns-1)*4+2], U1_n[:,(NumDesigns-1)*4+2], U2_n[:,(NumDesigns-1)*4+2], \\\n",
    "                                  V_n[:,(NumDesigns-1)*4+2],  \\\n",
    "                                  U1_n[:,(NumDesigns-1)*4+2] + V_n[:,(NumDesigns-1)*4+2]]).T, axis=0))\n",
    "\n",
    "print(n**(3/2)*np.diag(np.cov(np.vstack([S_n[:,(NumDesigns-1)*4+2], U1_n[:,(NumDesigns-1)*4+2], U2_n[:,(NumDesigns-1)*4+2], \\\n",
    "                                         V_n[:,(NumDesigns-1)*4+2], \\\n",
    "                                         U1_n[:,(NumDesigns-1)*4+2] + V_n[:,(NumDesigns-1)*4+2]]).T, \\\n",
    "                              rowvar = False, ddof = 1))**(1/2))\n",
    "\n",
    "print(\" \")\n",
    "t = np.vstack([tS_n[:,(NumDesigns-1)*4+2], tU1_n[:,(NumDesigns-1)*4+2],  tU2_n[:,(NumDesigns-1)*4+2], \\\n",
    "               tV_n[:,(NumDesigns-1)*4+2], tU1V_n[:,(NumDesigns-1)*4+2]]).T\n",
    "\n",
    "print(\"Tail frequences on standardized score components\")\n",
    "print(np.mean(t>=1.6449, axis = 0))\n",
    "print(np.mean(t<=-1.6449, axis = 0))\n",
    "print(np.mean(t>=1.96, axis = 0))\n",
    "print(np.mean(t<=-1.96, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f828d2",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "There is clearly a tremendous amount of additional research that could be done. Both in terms of exploring the accuracy of dense versus sparse asymptotic approximations across different designs and also in terms of evaluating the properties of different approaches/methods of interference. What is reported above represents just an initial pass. The results are sufficiently promising to suggest that sparse network asymptotics may be useful in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
